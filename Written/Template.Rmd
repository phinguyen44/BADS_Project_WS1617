---
title: "Fancy Title"
header-includes:
  - \usepackage{graphicx}
  - \usepackage{amsmath}
  - \usepackage{float}
  - \usepackage{hyperref}
  - \usepackage{setspace}
  - \onehalfspacing
output: pdf_document
latex_engine: lualatex
geometry: margin = 3.5cm
bibliography: references.bib
---

```{r, eval = TRUE, echo = FALSE, include = FALSE, cache = TRUE}
# source("C:/Users/Julian/cms/bachelor-thesis_Winkel/impl/voiproduction.R")

```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```
\pagenumbering{gobble}

<!-- \newcommand{\quantnet}{\hspace*{\fill} \raisebox{-1pt}{\includegraphics[scale=0.05]{C:/Users/Julian/cms/bachelor-thesis_Winkel/written/thesis/qletlogo.pdf}}\,} -->


\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
            {-2.5ex\@plus -1ex \@minus -.25ex}%
            {1.25ex \@plus .25ex}%
            {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}    

\bigskip

\begin{center}

Business Analytics and Data Science Group Project  

\medskip
submitted to  
\medskip



\textbf{First Prof} \linebreak
\textbf{Second Prof}  \linebreak


\medskip
Humboldt-Universit?t zu Berlin \linebreak     
School of Business and Economics  \linebreak
\medskip


Chair  \linebreak

by  \linebreak  

\textbf{Claudia Guenther, Phi Nguyen, Julian Winkel}  \linebreak  
Immatriculation Numbers \linebreak




\includegraphics[width=0.3\textwidth]{HU_Logo_small.png}

\medskip

Anything else we want to say

\medskip


Berlin, Date

\end{center}


\newpage
\pagenumbering{roman}



\begin{abstract}

\smallskip

Insert abstract here

\bigskip



\end{abstract}

\newpage

\listoftables

\newpage

\listoffigures

\newpage


\begin{Large}
\textbf{Abbreviations} \linebreak
\end{Large}

\begin{tabular}{ll}
\textbf{ANN} & Artificial Neural Network     \\
\textbf{LM} & Linear Model    \\

\end{tabular}


\newpage

\pagenumbering{arabic}

\tableofcontents

\newpage

\section{Introduction}


\newpage


\section{Previous Literature}
1 Page

Divide previous research in subsections that will be presented in the following.

This is how we cite @Badea_2014. The reference is automatically pasted in the according section.
You can also cite indirectly at the end of a sentence [@Badea_2014].
In this format, it is possible to insert pages, too [@Badea_2014, 10-14].


\clearpage
\newpage

\section{Methodoloy}
2 pages
\subsection{Predictive Analytics or other title}

\subsection{Ensemble Models}

\newpage


\clearpage

\newpage

\section{Data}
\subsection{Data sets}
The two data sets available to us contain a total of 150,000 order records from an online apparel retailer from a yearlong selling period. For 50,000 of these records it is unknown whether an ordered item has been sent back by the customer or not. This second data set is the subject of our binary predictions of customer’s returning behavior (return/not return). Both data sets include a total of 13 continuous and categorical variables.  These covariates give information on customer demographics (e.g. user state, date of birth, title), order details (e.g. order date, delivery date), and item characteristics (e.g. item size, price or color). To prepare the data sets for our analysis we apply a set of standard pre-processing actions. Following the careful inspection of each variable, we remove all implausible values (e.g. extreme outliers). Approximately 20% of all records have missing values in either the *delivery date* and *date of birth*. For bettter comprehensibility, we transform these variables *delivery time* and *age* respectively. Before imputing missing values, we flag them using dummy variables to extract their predictive power [@de2016advanced, 18].
Since *age* seems to be missing (completely) at random (MCAR) according to our data inspection, imputing it using mean substitution gives us an unbiased estimates [@schafer2002missing]^[Additionally, we carry out a Maximum Likelihood imputation of age in case the missing values are only missing at random (MAR), yielding the same model performance.]. 

Missing values in delivery time, caused by missing delivery dates, are clearly not missing not at random (MNAR) as they have a zero mean return rate and therefore are a perfect predictor. Possible reasons for these missing values are manifold. Without knowing the process generating these MNAR values, we cannot find unbiased substitutes form them [@schafer2002missing, 171]. We adopt three single substitution methods, namely case dropping, mean and median imputation, and chose the latter one based on model performance. We standardize the continuous variables only after the feature creation step to maintain their interpretability. Likewise, we do not directly drop zero (almost) zero variance predictors (e.g. date of birth) since we use them for feature extraction.
\subsection{Feature creation}
Creation of useful and informative features is the key to successful predictive models, as they allow a greater insight into data, improve prediction accuracy and can reduce excessive computational time for model estimations [@guyon2003introduction, 1158]. Based on the data structure available we separate our indicators into three e-commerce-related categories, namely product, customer and basket related features. Features in the product category are all item-specific indicators (e.g. item size, item price). Likewise, customer-related features contain variables with relational or demographic information on each customer (e.g. customer age, age of user account).  We define the basket-related features as those that are common within one order basket (e.g. number of items within a basket). We consider all items belonging to one basket that were ordered by the same user on the same day. 
In order to extract as much information as possible from the 13 original variables, we apply a variety of different extraction and projections methods.  This includes unsupervised methods (e.g. equal frequency binning) as well as supervised approaches, like mixed clustering (hierarchical and k-means). Our extraction methods can be broadly summarized in four groups: discretization, projection, transformation/combination of variables and data augmentation by using external information.
We apply discretization to several continuous variables, as this can be useful for capturing non-linear effects, constructing a more comprehensible representation of variables or improving predictive accuracy, especially in the context of tree-based decision algorithms [@liu2002discretization]. For example, we construct price item bins using equal frequency discretization as we suspect non-linearity regarding the target variable. This approach is superior to equal width discretization for our purpose since it is outlier-resistant and gives us a higher interval resolution for item prices with a high density [@cichosz2014data, 531]. 

A special challenge of this data set, which applies to many predictive modeling settings on customer behavior, is the presence of high-cardinality attributes (e.g. user id, item id). We replace each category with its Weight of evidence (WOE), a numerical projection method for categorical variables. The WOE is a measure of predictive power of a category with respect to the target variable, which has gained popularity in credit scoring [@de2016advanced, 19-20]. For a given customer, it shows a customers tendency in returning behavior, a valuable predictor variable. Replacing high-cardinality attributes with their respective WOE also reduces the dimensionality of our data set drastically. Since WOE has been shown to work well across the different types of our baseline models and requires low computational effort, we apply this projection methods to all categorical variables[@de2016advanced, 32-33]. 
# NOTE: Mention WOE ADJUSTMENT PROCEDURE


Another useful approach we take is the transformation and combination of variables based on domain knowledge. For example, we expect that discounted items will not only have a higher sales volume but also a lower return rate due to the financial incentive [@arnold2003hedonic]. Thus, we construct several discount variables (absolute discount, percentage discount, discount dummy) by taking the maximum item price for each item size as proxy for the regular item price and considering all deviations as a discount. Furthermore, a common defect of online shopping is that customers might be unsure about which size or color they should order based on the item description and thus place multiple orders of the same item [@foscht2013retaining]. We therefore construct informative variables on the order basket of consumers, e.g. the basket size in terms of items or the number of similar items with distinct sizes within one basket.
Furthermore, we augment and enrich the data sets by including external sources to construct new features. For example, we use statistics on the mean income for different age groups and different regions within Germany. By combining these indicators with each customer’s demographical data (age and user state) we are able construct a proxy for each customer’s income. Moreover, we tackle the lack of information on item categories (e.g. pants, shoes, clothing, accessories). Based on size tables for different apparel categories we gathered we are able to categorize over 90% of items by finding sizes unique to a category. As expected, the mean return rate of shoes is almost twice as high as mean returns in accessories.
\subsection{Feature selection}



\newpage

\section{Model building}
\subsection{Experimental design}
\subsubsection{Baseline models}
\subsubsection{Candidate selection and combination}
\subsection{Performance Measurement}
- discuss AUC, accuracy, costs
- post-processing

\clearpage
\newpage

\section{Results}


\section{Conclusion}


\newpage

\section{References}


<div id="refs"></div>


\newpage

\begin{Large}
\textbf{Declaration of Authorship}
\end{Large}

\bigskip
\bigskip

TEXT

\bigskip
\bigskip

15.01.2018
